{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thisisanu/online_deep_learning/blob/main/homework3/Homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bov3RYkG8VWh",
        "outputId": "93498193-958c-4ff2-f33d-991e3b32b09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 10 00:11:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8             10W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Ensure we are in /content\n",
        "os.chdir(\"/content\")\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "\n",
        "# If a folder named homework2 exists, remove it completely\n",
        "if os.path.exists(\"online_deep_learning\"):\n",
        "    shutil.rmtree(\"online_deep_learning\")\n",
        "    print(\"Old online_deep_learning folder removed.\")\n",
        "else:\n",
        "    print(\"No existing online_deep_learning folder found.\")\n",
        "\n",
        "!git clone https://github.com/thisisanu/online_deep_learning.git\n",
        "%cd online_deep_learning/homework3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekl-7BPR83PV",
        "outputId": "b0cffcfe-193a-4e04-fb6a-d47a173b1a71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content\n",
            "Old online_deep_learning folder removed.\n",
            "Cloning into 'online_deep_learning'...\n",
            "remote: Enumerating objects: 501, done.\u001b[K\n",
            "remote: Counting objects: 100% (206/206), done.\u001b[K\n",
            "remote: Compressing objects: 100% (198/198), done.\u001b[K\n",
            "remote: Total 501 (delta 137), reused 27 (delta 6), pack-reused 295 (from 2)\u001b[K\n",
            "Receiving objects: 100% (501/501), 3.06 MiB | 21.76 MiB/s, done.\n",
            "Resolving deltas: 100% (324/324), done.\n",
            "/content/online_deep_learning/homework3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy torch torchvision tqdm matplotlib\n",
        "\n",
        "#download the datasets by running the following command from the main directory\n",
        "!curl -s -L https://www.cs.utexas.edu/~bzhou/dl_class/classification_data.zip -o ./classification_data.zip && unzip -qo classification_data.zip\n",
        "!curl -s -L https://www.cs.utexas.edu/~bzhou/dl_class/drive_data.zip -o ./drive_data.zip && unzip -qo drive_data.zip\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybDIPCfnaOSf",
        "outputId": "96f5eec5-56df-4907-bced-e1d9aba10123"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "bundle.py\t\t drive_data\t homework\t  requirements.txt\n",
            "classification_data\t drive_data.zip  Homework3.ipynb\n",
            "classification_data.zip  grader\t\t README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!python3 -m grader homework -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFiQJezGcs1u",
        "outputId": "c0dd6b28-be9c-4034-d157-d52b10a64c63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bundle.py\t\t drive_data\t homework\t  requirements.txt\n",
            "classification_data\t drive_data.zip  Homework3.ipynb\n",
            "classification_data.zip  grader\t\t README.md\n",
            "Public grader loaded.\n",
            "\u001b[97m[INFO     00:00:001] \u001b[0m\u001b[97mClassifier\u001b[0m\n",
            "\u001b[97m[INFO     00:00:501] \u001b[0m\u001b[97m  - Predict                                            [ 10 / 10 ]\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:531] \u001b[0m\u001b[33m  - Accuracy                                           [ 0 / 25 ]\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:531] \u001b[0m\u001b[33mclassifier.th not found\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:555] \u001b[0m\u001b[33m  - Accuracy: Extra Credit                             [ 0 / 2 ]\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:555] \u001b[0m\u001b[33mclassifier.th not found\u001b[0m\n",
            "\u001b[97m[INFO     00:00:556] \u001b[0m\u001b[97m --------------------------------------------------    [  10 /  35 ]\u001b[0m\n",
            "\u001b[97m[INFO     00:00:557] \u001b[0m\u001b[97mDetector\u001b[0m\n",
            "\u001b[97m[INFO     00:00:603] \u001b[0m\u001b[97m  - Predict                                            [ 10 / 10 ]\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:604] \u001b[0m\u001b[33m  - Segmentation Accuracy                              [ 0 / 10 ]\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:604] \u001b[0m\u001b[33mdetector.th not found\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:604] \u001b[0m\u001b[33m  - Segmentation IoU                                   [ 0 / 25 ]\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:604] \u001b[0m\u001b[33mdetector.th not found\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:604] \u001b[0m\u001b[33m  - Segmentation IoU: Extra Credit                     [ 0 / 2 ]\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:604] \u001b[0m\u001b[33mdetector.th not found\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:604] \u001b[0m\u001b[33m  - Depth Error                                        [ 0 / 10 ]\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:604] \u001b[0m\u001b[33mdetector.th not found\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:605] \u001b[0m\u001b[33m  - Depth Error: Extra Credit                          [ 0 / 2 ]\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:605] \u001b[0m\u001b[33mdetector.th not found\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:605] \u001b[0m\u001b[33m  - True Positives Depth Error                         [ 0 / 10 ]\u001b[0m\n",
            "\u001b[33m[WARNING  00:00:605] \u001b[0m\u001b[33mdetector.th not found\u001b[0m\n",
            "\u001b[97m[INFO     00:00:605] \u001b[0m\u001b[97m --------------------------------------------------    [  10 /  65 ]\u001b[0m\n",
            "\u001b[97m[INFO     00:00:605] \u001b[0m\u001b[97mTotal                                                     20 / 100\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "101b4b4b"
      },
      "source": [
        "# Task\n",
        "Implement and train a classification model, saving the trained model as `classifier.th`. Subsequently, implement and train an object detection model, saving the trained model as `detector.th`. Conclude by confirming the successful generation of both `classifier.th` and `detector.th` files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c873d884"
      },
      "source": [
        "## Implement Classification Training Setup\n",
        "\n",
        "### Subtask:\n",
        "Create the `train_classification.py` script and set up initial components like imports, device configuration, and data loading for the classification task. This will prepare the environment for model definition and training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14eb21f4"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires creating a new Python file `train_classification.py` and adding initial setup code for imports, device configuration, data transformations, and data loading. This code block will create the file and include all the necessary components as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "4574dd07",
        "outputId": "29811ba0-7923-4bb2-e05f-48f27f5ccb50"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "# 1. Device configuration\n",
        "def setup_device():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    return device\n",
        "\n",
        "# 2. Data transformations\n",
        "def get_data_transforms(image_size):\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "    return data_transforms\n",
        "\n",
        "# 3. Data loading\n",
        "def get_data_loaders(data_dir, data_transforms, batch_size):\n",
        "    image_datasets = {\n",
        "        x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "        for x in ['train', 'val']\n",
        "    }\n",
        "    dataloaders = {\n",
        "        x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        for x in ['train', 'val']\n",
        "    }\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "    print(f\"Found {dataset_sizes['train']} training images and {dataset_sizes['val']} validation images.\")\n",
        "    print(f\"Classes: {class_names}\")\n",
        "    return dataloaders, dataset_sizes, class_names\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Classification Training Setup')\n",
        "    parser.add_argument('--data_dir', type=str, default='./classification_data', help='Directory with classification data')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for data loaders')\n",
        "    parser.add_argument('--image_size', type=int, default=224, help='Image size for resizing')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Create train_classification.py and write the setup code to it\n",
        "    file_content = f'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "# Device configuration\n",
        "def setup_device():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {{device}}\")\n",
        "    return device\n",
        "\n",
        "# Data transformations\n",
        "def get_data_transforms(image_size):\n",
        "    data_transforms = {{\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }}\n",
        "    return data_transforms\n",
        "\n",
        "# Data loading\n",
        "def get_data_loaders(data_dir, data_transforms, batch_size):\n",
        "    image_datasets = {{\n",
        "        x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "        for x in ['train', 'val']\n",
        "    }}\n",
        "    dataloaders = {{\n",
        "        x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        for x in ['train', 'val']\n",
        "    }}\n",
        "    dataset_sizes = {{x: len(image_datasets[x]) for x in ['train', 'val']}}\n",
        "    class_names = image_datasets['train'].classes\n",
        "    print(f\"Found {{dataset_sizes['train']}} training images and {{dataset_sizes['val']}} validation images.\")\n",
        "    print(f\"Classes: {{class_names}}\")\n",
        "    return dataloaders, dataset_sizes, class_names\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Classification Training Setup')\n",
        "    parser.add_argument('--data_dir', type=str, default='./classification_data', help='Directory with classification data')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for data loaders')\n",
        "    parser.add_argument('--image_size', type=int, default=224, help='Image size for resizing')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    device = setup_device()\n",
        "    data_transforms = get_data_transforms(args.image_size)\n",
        "    dataloaders, dataset_sizes, class_names = get_data_loaders(args.data_dir, data_transforms, args.batch_size)\n",
        "\n",
        "    # Example of how to access data\n",
        "    # inputs, labels = next(iter(dataloaders['train']))\n",
        "    # print(f\"Batch input shape: {{inputs.shape}}\")\n",
        "    # print(f\"Batch labels shape: {{labels.shape}}\")\n",
        "\n",
        "    print(\"Classification training setup complete.\")\n",
        "    '''\n",
        "\n",
        "    with open('train_classification.py', 'w') as f:\n",
        "        f.write(file_content)\n",
        "    print(\"Created train_classification.py with initial setup code.\")\n",
        "\n",
        "    # Optionally, run the script to test the setup (not part of the file content itself)\n",
        "    # print(\"\\nRunning train_classification.py to verify setup...\")\n",
        "    # os.system('python train_classification.py')\n",
        "\n",
        "    print(\"Initial classification training setup components are ready in train_classification.py.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--data_dir DATA_DIR]\n",
            "                                [--batch_size BATCH_SIZE]\n",
            "                                [--image_size IMAGE_SIZE]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-16a2b55d-7aa3-4641-aad2-09b2ce68c51b.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e6b0ba3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the `argparse.ArgumentParser().parse_args()` method was being called directly within the Jupyter cell, which attempts to parse arguments passed by the Jupyter kernel (like `-f`), leading to an `unrecognized arguments` error. The fix is to remove the `if __name__ == '__main__':` block and its associated `argparse` parsing from the current notebook cell, ensuring that the code inside this block (including `argparse`) is only part of the string written to `train_classification.py` and not executed by the cell itself.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c063a596",
        "outputId": "f814167d-d982-44e3-bf3f-b43cc8b47b09"
      },
      "source": [
        "import os\n",
        "\n",
        "# The content that will be written to train_classification.py\n",
        "# This string contains all the necessary imports, function definitions,\n",
        "# and the main execution block for the classification training setup.\n",
        "file_content = \"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "# Device configuration\n",
        "def setup_device():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    return device\n",
        "\n",
        "# Data transformations\n",
        "def get_data_transforms(image_size):\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "    return data_transforms\n",
        "\n",
        "# Data loading\n",
        "def get_data_loaders(data_dir, data_transforms, batch_size):\n",
        "    image_datasets = {\n",
        "        x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "        for x in ['train', 'val']\n",
        "    }\n",
        "    dataloaders = {\n",
        "        x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        for x in ['train', 'val']\n",
        "    }\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "    print(f\"Found {dataset_sizes['train']} training images and {dataset_sizes['val']} validation images.\")\n",
        "    print(f\"Classes: {class_names}\")\n",
        "    return dataloaders, dataset_sizes, class_names\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Classification Training Setup')\n",
        "    parser.add_argument('--data_dir', type=str, default='./classification_data', help='Directory with classification data')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for data loaders')\n",
        "    parser.add_argument('--image_size', type=int, default=224, help='Image size for resizing')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    device = setup_device()\n",
        "    data_transforms = get_data_transforms(args.image_size)\n",
        "    dataloaders, dataset_sizes, class_names = get_data_loaders(args.data_dir, data_transforms, args.batch_size)\n",
        "\n",
        "    # Example of how to access data\n",
        "    # inputs, labels = next(iter(dataloaders['train']))\n",
        "    # print(f\"Batch input shape: {inputs.shape}\")\n",
        "    # print(f\"Batch labels shape: {labels.shape}\")\n",
        "\n",
        "    print(\"Classification training setup complete.\")\n",
        "\"\"\"\n",
        "\n",
        "# Write the content to the file\n",
        "with open('train_classification.py', 'w') as f:\n",
        "    f.write(file_content)\n",
        "print(\"Created train_classification.py with initial setup code.\")\n",
        "print(\"Initial classification training setup components are ready in train_classification.py.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created train_classification.py with initial setup code.\n",
            "Initial classification training setup components are ready in train_classification.py.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93d47b97"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps successfully created the `train_classification.py` file. Now, to verify its functionality and ensure the initial setup components (device config, data transforms, data loading) are correctly integrated, I will execute the script. This will confirm that the file is executable and performs the expected setup operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65135037"
      },
      "source": [
        "import os\n",
        "\n",
        "print(\"Verifying train_classification.py setup...\")\n",
        "!python train_classification.py --data_dir ./classification_data\n",
        "print(\"Verification complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}